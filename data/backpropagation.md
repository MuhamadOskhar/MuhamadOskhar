Backpropagation merupakan teknik yang sering digunakan dalam machine learning dan deep learning untuk melakukan optimisasi pada model yang sedang dilatih. Selain kalkulus, ada beberapa cabang ilmu yang berperan penting dalam backpropagation, antara lain:

`Aljabar linear`: Aljabar linear digunakan untuk memodelkan masalah matematika yang melibatkan matriks dan vektor, yang seringkali muncul dalam deep learning.

`Teori probabilitas`: Teori probabilitas digunakan untuk memodelkan ketidakpastian dalam data dan parameter model, serta untuk memperkirakan distribusi probabilitas dari output model.

`Optimisasi`: Optimisasi digunakan untuk mencari nilai optimum dari fungsi objektif yang digunakan dalam training model. Algoritma optimisasi seperti gradient descent sering digunakan dalam backpropagation.

`Teori graf`: Backpropagation melibatkan representasi model dengan graf, sehingga teori graf juga berperan penting dalam teknik ini.

`Statistik`: Statistik digunakan dalam analisis data dan pembuatan keputusan berdasarkan data yang tersedia, termasuk dalam pemilihan model yang paling sesuai dengan data.

Jadi, dapat dikatakan bahwa backpropagation melibatkan beberapa cabang ilmu yang berbeda, dan bukan hanya terbatas pada kalkulus.

Dalam backpropagation, kalkulus memiliki beberapa bagian yang berperan penting, antara lain:

`Turunan (derivative)`: turunan digunakan untuk menghitung perubahan gradien suatu fungsi terhadap parameter tertentu. Dalam backpropagation, turunan digunakan untuk menghitung gradien kesalahan (error) suatu model terhadap bobot (weights) dan bias (biases) di setiap layer.

`Fungsi aktivasi (activation function)`: fungsi aktivasi adalah fungsi yang digunakan untuk mengubah input pada suatu neuron menjadi output yang akan diteruskan ke neuron selanjutnya. Fungsi aktivasi memiliki turunan, yang diperlukan dalam menghitung gradien kesalahan pada layer sebelumnya.

`Chain rule`: aturan rantai (chain rule) adalah prinsip dasar kalkulus yang digunakan untuk menghitung turunan fungsi komposit. Dalam backpropagation, aturan rantai digunakan untuk menghitung turunan fungsi kesalahan terhadap bobot dan bias di setiap layer.

Dalam kombinasi, ketiga bagian kalkulus ini memungkinkan backpropagation untuk menghitung gradien kesalahan pada setiap layer, dan menggunakan gradien tersebut untuk memperbarui bobot dan bias agar model dapat belajar dari data.

Ketiga bagian kalkulus yang disebutkan (turunan, fungsi aktivasi, dan aturan rantai) merupakan elemen penting dalam backpropagation dan tidak dapat diabaikan satu sama lain. Namun, jika harus memilih yang paling wajib dan pokok, maka turunan akan menjadi elemen yang paling penting dalam backpropagation.

Turunan digunakan untuk menghitung gradien kesalahan model terhadap setiap bobot dan bias di setiap layer. Gradien kesalahan ini diperlukan untuk memperbarui bobot dan bias pada setiap iterasi pelatihan. Tanpa turunan, tidak mungkin untuk menghitung gradien kesalahan dan mengoptimalkan model melalui pelatihan.

Fungsi aktivasi dan aturan rantai juga penting dalam backpropagation, namun keduanya memainkan peran tambahan dalam menghitung gradien kesalahan pada setiap layer. Fungsi aktivasi digunakan untuk mengubah input pada neuron menjadi output yang akan diteruskan ke layer selanjutnya dan memengaruhi gradien kesalahan pada layer tersebut. Aturan rantai digunakan untuk menghitung turunan fungsi kesalahan terhadap bobot dan bias di setiap layer. Keduanya diperlukan dalam menghitung gradien kesalahan secara efisien, namun turunan tetap menjadi elemen yang paling penting.